{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0134566 , -0.02709758, -0.03354747, -0.00018994])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env = gym.make('LunarLander-v2')\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.06697994, -0.98592291, -0.01881073,  1.09356332]), 1.0, False, {})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created by Andrew Silva on 8/28/19\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from interpretable_ddts.agents.ddt_agent import DDTAgent\n",
    "from interpretable_ddts.agents.mlp_agent import MLPAgent\n",
    "from interpretable_ddts.opt_helpers.replay_buffer import discount_reward\n",
    "import torch.multiprocessing as mp\n",
    "import argparse\n",
    "import copy\n",
    "import random\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def run_episode(q, agent_in, ENV_NAME, seed=0):\n",
    "    agent = agent_in.duplicate()\n",
    "    if ENV_NAME == 'lunar':\n",
    "        env = gym.make('LunarLander-v2')\n",
    "    elif ENV_NAME == 'cart':\n",
    "        env = gym.make('CartPole-v1')\n",
    "    else:\n",
    "        raise Exception('No valid environment selected')\n",
    "    done = False\n",
    "    torch.manual_seed(seed)\n",
    "    env.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "    random.seed(seed)\n",
    "    state = env.reset()  # Reset environment and record the starting state\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        # Step through environment using chosen action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        # env.render()\n",
    "        # Save reward\n",
    "        agent.save_reward(reward)\n",
    "        if done:\n",
    "            break\n",
    "    reward_sum = np.sum(agent.replay_buffer.rewards_list)\n",
    "    rewards_list, advantage_list, deeper_advantage_list = discount_reward(agent.replay_buffer.rewards_list,\n",
    "                                                                          agent.replay_buffer.value_list,\n",
    "                                                                          agent.replay_buffer.deeper_value_list)\n",
    "    agent.replay_buffer.rewards_list = rewards_list\n",
    "    agent.replay_buffer.advantage_list = advantage_list\n",
    "    agent.replay_buffer.deeper_advantage_list = deeper_advantage_list\n",
    "\n",
    "    to_return = [reward_sum, copy.deepcopy(agent.replay_buffer.__getstate__())]\n",
    "    if q is not None:\n",
    "        try:\n",
    "            q.put(to_return)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "            return to_return\n",
    "    return to_return\n",
    "\n",
    "\n",
    "def main(episodes, agent, ENV_NAME):\n",
    "    running_reward_array = []\n",
    "    if not os.path.exists('../models/'):\n",
    "        os.mkdir('../models/')\n",
    "    for episode in tqdm(range(episodes), desc='episode'):\n",
    "        reward = 0\n",
    "        returned_object = run_episode(None, agent_in=agent, ENV_NAME=ENV_NAME)\n",
    "        reward += returned_object[0]\n",
    "        running_reward_array.append(returned_object[0])\n",
    "        agent.replay_buffer.extend(returned_object[1])\n",
    "        if reward >= 499:\n",
    "            agent.save('../models/'+str(episode)+'th')\n",
    "        agent.end_episode(reward)\n",
    "\n",
    "        running_reward = sum(running_reward_array[-100:]) / float(min(100.0, len(running_reward_array)))\n",
    "        if episode % 50 == 0:\n",
    "            print(f'Episode {episode}  Last Reward: {reward}  Average Reward: {running_reward}')\n",
    "        if episode % 500 == 0:\n",
    "            agent.save('../models/'+str(episode)+'th')\n",
    "\n",
    "    return running_reward_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['testrun',\n",
    "            '-gpu',\n",
    "            '-env', 'lunar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input arguments Namespace(agent_type='ddt', env_type='lunar', episodes=1000, gpu=True, num_hidden=0, num_leaves=4)\n",
      "Agent ddt on lunar \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b961df70f2444be0bf55993feebc133c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='episode', max=1000, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0  Last Reward: -141.38449639721102  Average Reward: -141.38449639721102\n",
      "Episode 50  Last Reward: -268.3299956986182  Average Reward: -239.57945563274947\n",
      "Episode 100  Last Reward: -661.7041534927689  Average Reward: -380.84048331829007\n",
      "Episode 150  Last Reward: -803.439844655393  Average Reward: -614.1282171745711\n",
      "Episode 200  Last Reward: -880.9674847152235  Average Reward: -776.4900365617389\n",
      "Episode 250  Last Reward: -692.7540027543866  Average Reward: -853.4683804628032\n",
      "Episode 300  Last Reward: -835.3598241380892  Average Reward: -857.0173753661479\n",
      "Episode 350  Last Reward: -871.0311845460166  Average Reward: -855.729001404035\n",
      "Episode 400  Last Reward: -859.8061183577315  Average Reward: -840.4170041944824\n",
      "Episode 450  Last Reward: -865.5074692261323  Average Reward: -839.8493667578606\n",
      "Episode 500  Last Reward: -994.5552361582555  Average Reward: -869.7882886691809\n",
      "Episode 550  Last Reward: -939.871232992491  Average Reward: -898.3459178467033\n",
      "Episode 600  Last Reward: -717.6023383521422  Average Reward: -891.0768818732524\n",
      "Episode 650  Last Reward: -956.3482383339946  Average Reward: -894.1713848595115\n",
      "Episode 700  Last Reward: -980.2476944115958  Average Reward: -928.4917735641264\n",
      "Episode 750  Last Reward: -841.3544481152538  Average Reward: -938.9594662151276\n",
      "Episode 800  Last Reward: -858.2256151742794  Average Reward: -900.2350899318682\n",
      "Episode 850  Last Reward: -885.4801736860989  Average Reward: -859.8447999791065\n",
      "Episode 900  Last Reward: -885.4801736860989  Average Reward: -861.4980669786137\n",
      "Episode 950  Last Reward: -669.841177435464  Average Reward: -843.2374351920024\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c19c948e1643b79c9b1283d0ef8247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='episode', max=1000, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0  Last Reward: -141.38449639721102  Average Reward: -141.38449639721102\n",
      "Episode 50  Last Reward: -268.3299956986182  Average Reward: -239.57945563274947\n",
      "Episode 100  Last Reward: -661.7041534927689  Average Reward: -380.84048331829007\n",
      "Episode 150  Last Reward: -803.439844655393  Average Reward: -614.1282171745711\n",
      "Episode 200  Last Reward: -880.9674847152235  Average Reward: -776.4900365617389\n",
      "Episode 250  Last Reward: -692.7540027543866  Average Reward: -853.4683804628032\n",
      "Episode 300  Last Reward: -835.3598241380892  Average Reward: -857.0173753661479\n",
      "Episode 350  Last Reward: -871.0311845460166  Average Reward: -855.729001404035\n",
      "Episode 400  Last Reward: -859.8061183577315  Average Reward: -840.4170041944824\n",
      "Episode 450  Last Reward: -865.5074692261323  Average Reward: -839.8493667578606\n",
      "Episode 500  Last Reward: -994.5552361582555  Average Reward: -869.7882886691809\n",
      "Episode 550  Last Reward: -939.871232992491  Average Reward: -898.3459178467033\n",
      "Episode 600  Last Reward: -717.6023383521422  Average Reward: -891.0768818732524\n",
      "Episode 650  Last Reward: -956.3482383339946  Average Reward: -894.1713848595115\n",
      "Episode 700  Last Reward: -980.2476944115958  Average Reward: -928.4917735641264\n",
      "Episode 750  Last Reward: -841.3544481152538  Average Reward: -938.9594662151276\n",
      "Episode 800  Last Reward: -858.2256151742794  Average Reward: -900.2350899318682\n",
      "Episode 850  Last Reward: -885.4801736860989  Average Reward: -859.8447999791065\n",
      "Episode 900  Last Reward: -885.4801736860989  Average Reward: -861.4980669786137\n",
      "Episode 950  Last Reward: -669.841177435464  Average Reward: -843.2374351920024\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e728dbb02543828704f4183c61e488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='episode', max=1000, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0  Last Reward: -141.38449639721102  Average Reward: -141.38449639721102\n",
      "Episode 50  Last Reward: -268.3299956986182  Average Reward: -239.57945563274947\n",
      "Episode 100  Last Reward: -661.7041534927689  Average Reward: -380.84048331829007\n",
      "Episode 150  Last Reward: -803.439844655393  Average Reward: -614.1282171745711\n",
      "Episode 200  Last Reward: -880.9674847152235  Average Reward: -776.4900365617389\n",
      "Episode 250  Last Reward: -692.7540027543866  Average Reward: -853.4683804628032\n",
      "Episode 300  Last Reward: -835.3598241380892  Average Reward: -857.0173753661479\n",
      "Episode 350  Last Reward: -871.0311845460166  Average Reward: -855.729001404035\n",
      "Episode 400  Last Reward: -859.8061183577315  Average Reward: -840.4170041944824\n",
      "Episode 450  Last Reward: -865.5074692261323  Average Reward: -839.8493667578606\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-a\", \"--agent_type\", help=\"architecture of agent to run\", type=str, default='ddt')\n",
    "    parser.add_argument(\"-e\", \"--episodes\", help=\"how many episodes\", type=int, default=1000)\n",
    "    parser.add_argument(\"-l\", \"--num_leaves\", help=\"number of leaves for DDT/DRL \", type=int, default=4)\n",
    "    parser.add_argument(\"-n\", \"--num_hidden\", help=\"number of hidden layers for MLP \", type=int, default=0)\n",
    "    parser.add_argument(\"-env\", \"--env_type\", help=\"environment to run on\", type=str, default='cart')\n",
    "    parser.add_argument(\"-gpu\", help=\"run on GPU?\", action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(f\"input arguments {args}\")\n",
    "    \n",
    "    \n",
    "    AGENT_TYPE = args.agent_type  # 'ddt', 'mlp'\n",
    "    NUM_EPS = args.episodes  # num episodes Default 1000\n",
    "    ENV_TYPE = args.env_type  # 'cart' or 'lunar' Default 'cart'\n",
    "    USE_GPU = args.gpu  # Applies for 'prolo' only. use gpu? Default false\n",
    "    if ENV_TYPE == 'lunar':\n",
    "        init_env = gym.make('LunarLander-v2')\n",
    "        dim_in = init_env.observation_space.shape[0]\n",
    "        dim_out = init_env.action_space.n\n",
    "    elif ENV_TYPE == 'cart':\n",
    "        init_env = gym.make('CartPole-v1')\n",
    "        dim_in = init_env.observation_space.shape[0]\n",
    "        dim_out = init_env.action_space.n\n",
    "    else:\n",
    "        raise Exception('No valid environment selected')\n",
    "\n",
    "    print(f\"Agent {AGENT_TYPE} on {ENV_TYPE} \")\n",
    "    # mp.set_start_method('spawn')\n",
    "    mp.set_sharing_strategy('file_system')\n",
    "    for i in range(5):\n",
    "        bot_name = AGENT_TYPE + '_' + ENV_TYPE\n",
    "        if USE_GPU:\n",
    "            bot_name += '_GPU_'\n",
    "        if AGENT_TYPE == 'ddt':\n",
    "            policy_agent = DDTAgent(bot_name=bot_name,\n",
    "                                    input_dim=dim_in,\n",
    "                                    output_dim=dim_out,\n",
    "                                    rule_list=False,\n",
    "                                    num_rules=args.num_leaves)\n",
    "        elif AGENT_TYPE == 'mlp':\n",
    "            policy_agent = MLPAgent(input_dim=dim_in,\n",
    "                                    bot_name=bot_name,\n",
    "                                    output_dim=dim_out,\n",
    "                                    num_hidden=args.num_hidden)\n",
    "        else:\n",
    "            raise Exception('No valid network selected')\n",
    "        reward_array = main(NUM_EPS, policy_agent, ENV_TYPE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
